# data-engineering-fullstack
This repository contains links to all resources for full stack data engineering skillset

<h3> List of Target Companies </h3>

* [Razorpay] (https://razorpay.com/?utm_source=linkedin&utm_medium=social) [Job_Description](https://www.linkedin.com/jobs/view/2631333149)
    * ranked in linkedin top startups
    * neelesh told to get knowledge of financial domain, trivia and stuff also
    * Experience building and optimizing **‘big data’ data pipelines, architectures and data sets**.
    * Strong **analytic skills** related to working with structured/unstructured datasets.
    * Build processes supporting data transformation, data structures, **dimensional modelling, metadata, dependency, schema registration/evolution and workload managemen**t.
    * Working knowledge of **message queuing, stream processing**, and highly scalable ‘big data’ data stores.
    * Experience supporting and working with cross-functional teams in a dynamic environment.
    * We are looking for a candidate with 4+ years of experience in a Data Engineer role. They should also have experience using the following software/tools:
        * Experience with big data tools: **HDFS/S3, Spark/Flink,Hive,Hbase, Kafka/kanisis**, etc.
        * Experience with relational SQL and NoSQL databases, including **Elasticsearch and Cassandra/Mongodb**.
        * Experience with data pipeline and workflow management tools: **Azkaban, Luigi, Airflow**, etc.
        * Experience with **AWS** /GCP cloud services
        * Experience with stream-processing systems: **Spark-Streaming/Flink** etc.
        * Experience with object-oriented/object function scripting languages: **Java, Scala**, etc.

* [Disruptors_Capital](http://www.disruptors.capital/) [Job_Description](https://www.linkedin.com/jobs/view/2850688238)
    * Very small company, recently started, less than 100 employees, investment company
    * Experience building scalable, real time and high-performance data lake solutions usingtech such **Kafka**, **AWS EMR, S3, Hive, Spark**- Experience with workflow scheduling tools like **Airflow**- Experience with relational and non-relational databases- Experience with scripting languages such as Shell, Python- Programming Languages: **Java / Python / Scala**- Experience working with micro service architecture- Experience working with cloud-based service providers (preferably AWS)- Knowledge of **linear and logistic regression, K-means, K-medoids, Naive Bayes, Decision tree, Random forest, Clustering, Bayesian model, CNN, Transformers** would be considered a plus- Knowledge of natural language processing (**NLP**) will be considered a plus
* [Walmart_Global_Tech_India](https://one.walmart.com/content/globaltechindia/en_in.html) [Job_Description](https://www.linkedin.com/jobs/view/2848676337) 
    * Bachelor's Degree or Master’s Degree with **3+ years** of experience in Computer Science or related field.
    * Experience in **ETL** and Expertise in **SQL**.
    * Expertise in Big Data Ecosystem with experience in **Hadoop, Hive, Spark, Strome, Cassandra, NoSQL DB’s**.
    * Expertise in **MPP architecture** and knowledge of MPP engine (Spark, **Impala** etc).
    * Data pipeline/workflow management tools such as **Azkaban, Airflow and oozie**
    * **Cloud** Development experience
    * Experience in building scalable/highly available distributed systems in production.
    * Understanding of **stream processing** with knowledge on **Kafka**.
    * Knowledge of Software Engineering best practices with experience on implementing CI/CD, Log aggregation/* * Monitoring/alerting for production system.
    * Very good expertise in production support related activities (issue identification, resolution)
* [Goldman_Sachs](http://www.goldmansachs.com/) [Job_Description_1](https://www.linkedin.com/jobs/view/2813599926) [Job_Description_2](https://www.linkedin.com/jobs/view/2814202082)
    * Advanced degree in Engineering, Computer Science or related disciplines
    * Hands-on experience with one or more mainstream programming languages (**Python, Java, C, Scala, C++**, etc.)
    * Hands-on experience with one or more relational database systems (**Sybase, Snowflake, Oracle**)
    * Experience in building data products from ideation to implementation
    * At least 2 years experience in a visualization tool such as **Tableau / QlikView / Spotfire / Power BI**
    * At least 3 years experience dealing with data (structured or unstructured)
    * Ability to use data for business analysis and to drive customer focussed ideas
    * Strong project management skills
    * Experience working in a start-up business or a new business line within a larger organization is preferred

* [Twilio](http://www.twilio.com/) [Job_Description](https://www.linkedin.com/jobs/view/2848216345) 
    * Product based (Chat App), IPO done, 11 rounds funding done
    * Minimum of 3+ years of technical experience supporting highly-available Data warehouse systems and ETLs.
    * Hands-on strong experience with **SQL/NoSQL** databases. A good understanding of troubleshooting highly **scalable Data warehouses and Data pipelines**.
    * Hands on Experience in **SQL , Python and Scala** programming languages.
    * Experience in supporting distributed environments using **Spark, Hive, Presto, Hadoop** etc.
    * Deep understanding of architecture and functioning of Distributed database systems like **Snowflake, Presto and Redshift** (any ) .
    * Experience working with various file formats like **Parquet, Avro, ORC** for large volumes of data
    * Experience with one or more NoSQL databases such as **Cassandra, MongoDB, DynamoDB** is a plus.
    * Strong understanding of engineering standard methodologies design principles
    * Experience working in agile environment and iterative development

* [Baker_Hughes](http://www.bakerhughes.com/) [Job_Description](https://www.linkedin.com/jobs/view/2849489228)
    * Baker Hughes (NASDAQ: BKR) is an energy technology company that provides solutions for energy and industrial customers worldwide.
    * Be a Graduate in Computer Engineering. A minimum 3 yrs of professional experience
    * Have minimum of 3 yrs of development in **Informatica or Talend**
    * Have minimum of 2 yrs of writing **advanced SQL and stored procedures**
    * Have Understanding of logical and physical **data models and data lineage**
    * Have good experience with SQL Database such as **Oracle MySQL, PostgreSQL**
    * Have hands-on experience with **star schema models** to convert and derive flat summary data models for analytics and summary statistics
    * Have experience with **cloud** technologies and open source tools like **python, spark, Java**
    * Have experience on new D&A application development and existing application optimization is required (data source identification, mapping, ingestion, modelling and transformation for analytical consumption purposes).

* [Hitachi_Vantara](Hitachi Vantara) [Job_Description](https://www.linkedin.com/jobs/view/2850487966)
    * wholly-owned subsidiary of Hitachi, Ltd., guides our customers from what’s now to what’s next by solving their digital challenges, More than 80% of the Fortune 100 trust Hitachi Vantara to help them develop new revenue stream
    * 5+ years' experience working in data & analytics ecosystem, working with structured, semi-structured and unstructured data.
    * Demonstrated experience with **Data Pipeline development** both on-premises and in the cloud (AWS/Azure) using open-source technology
    * Responsible for creating reusable and scalable data pipelines using **cloud ETL tools & technology like Azure Data Factory, Glue etc.**
    * Sound understanding of various data solution patterns and when to use them: **ETL/ELT, RDBMS, Normalization/De-normalization, Key-Value, In-Memory, Wide Column, Columnar, Text Indexing, Streaming, Messaging**
    * Strong software engineering and object-oriented programming skills with expertise in languages such as **SQL, Python.**
    * Strong **data warehousing, data modeling and data manipulation skills**, such as in SQL, Stored Procedures, SQL Server
    * Experience with Agile methodologies, such as Scrum, Kanban, Lean
    * Experience building data pipelines using **Databricks**.
    * Experience building on emerging cloud serverless managed services, to minimize/eliminate physical/virtual server footprint
    * Experience implementing security solutions for data storage and processing in the cloud

* [Zoom](https://www.zoom.us/) [Job_Description](https://www.linkedin.com/jobs/view/2848214174)
    * famous video call platform 
    * jd not given

* [Expedia_Group](http://www.lifeatexpediagroup.com/) [Job_Description](https://www.linkedin.com/jobs/view/2849027140)
    * travel company (product based)
    * this jd for 8 years experience

* [Trell](https://trell.co.in/) [Job_Description](https://www.linkedin.com/jobs/view/2848222710)
    * linkedin ranked top startup
    -Experience in building production big-data crunching systems (**Spark, Kafka, HIVE, Flink** etc)
    * Strong data architectural skills.
    * Strong problem-solving skills.
    * Knowledge of distributed computing using **Spark**.
    * Ability to work in cross-functional projects and drive decision making.
    * Ability to wrangle unstructured data, as per use cases.
    * Experience schema design, data modelling and manipulating large data sets using both relational (e.g., **MySQL Postgres**) and non-relational databases (e.g., **MongoDB, CouchDB, Redis, Cassandra**).
    * Ability to consume, transform and optimize complex SQL queries.
    * Solid knowledge in python and data structures & a software engineering mindset, striving to write elegant, maintainable code.
    * Reliable knowledge/experience in consuming Web Services.
    * Knowledge/experience in consuming **real-time data streams**.
    * Knowledge of data warehousing (**BigQuery, Redshift, Snowflake** etc.) and working with data lakes (**S3, ADLS**).
    * Knowledge of **statistical** and **machine learning** models and preprocessing techniques would be an added advantage

* Thermo Fisher [Job_Description](https://www.linkedin.com/jobs/view/2848250066)
    * Having Certifications like **AWS Certified Data Analytics, CCA Spark and Hadoop Developer or CCP Data Engineer** is highly desirable
    * 5+ Years of Overall IT Experience and at least 3+ years as AWS Data Engineer in Data Lake, Data Analytics & Business Intelligence Solutions
    * Full life cycle project implementation experience in AWS using **Pyspark/EMR, Athena, S3, Redshift, AWS API Gateway, Lambda, Glue** and other managed services
    * Strong experience in building ETL data pipelines using Pyspark on EMR framework
    * Hands on experience in using S3, AWS Glue jobs, S3 Copy, Lambda and API Gateway.
    * Working SQL experience to troubleshoot SQL code. Redshift knowledge is an added advantage.
    * Hands-on with system & application log tools like **Datadog, CloudWatch, Splunk** etc.
    * Experience working with Python, **Python ML libraries** for data analysis, wrangling and insights generation
    * Strong understanding of **AWS Data lake and data bricks**.
    * Exposure to **Kafka, Redshift, Sage Maker** would be added advantage
    * Exposure to data visualization tools like **Power BI, Tableau** etc.

* [Grofers](http://www.blinkit.in/) [Job_Description](https://www.linkedin.com/jobs/view/2848216923)
    * 6 to 8 years of relevant industry experience.
    * Bachelor's and/or Master's degree in CS, or equivalent experience.
    * Solid knowledge of Computer Science fundamentals (object-oriented design, **data structures** and algorithm design, problem-solving, and complexity analysis).
    * A deep understanding of programming with one or more programming languages.
    * Python and Scala are preferred.
    * Experience designing, building and operating distributed systems, deploying high-performance systems with reliable monitoring and logging practices
    * Experience operating big data systems for large scale storage and processing of data.
    * Experience leading projects and mentoring members in the team.
    * Effectively work across team boundaries to establish overarching data architecture, and provide guidance to individual teams.
    * In-depth knowledge of relational and non-relational databases.
    * Take part in code/design reviews to further develop your sense of code quality, maintainability, test coverage, and good design
    * **Data Analysis,Data Streaming,NoSQL,Search,Kafka**

* [Agoda](http://careersatagoda.com/) [Job_Description](https://www.linkedin.com/jobs/view/2758702161)
    * Bangkok based job, online travel platform
    * Over 2 years of hands-on experience working with technology like Spark, Hadoop, Kafka
    * Knowledge and experience with machine learning
    * Experience with Scrum/Agile development methodologies
    * Experience building large-scale distributed products
* American_Express [Job_Description](https://www.linkedin.com/jobs/view/2766245216)
* Visa [Job_Description](https://www.linkedin.com/jobs/view/2814744862)
* slice
* [Bosch](http://www.boschindia.com/) [Job_Description](https://www.linkedin.com/jobs/view/2849256751)
    * Hands on experience on **Spark sql and Spark streaming**.
    * Hands on experience on **Scala** language.
    * In depth knowledge of **SQL** and **NoSQL** (HBase).
    * Good in writing Shell and **python** script.
    * Good understanding of **Hadoop ecosystem**.
    * Hands on experience on InteliJ, Github /Bitbucket, HUE.
    * Understanding of **ETL** proces

* [Acko](http://www.acko.com/) [Job_Description](https://www.linkedin.com/jobs/view/2818948287)
    * product based, insurance, d-series funding
    * Design, develop and maintain scalable data pipelines with a focus on writing clean, fault-tolerant code using Python, Airflow, Kafka, Spark, Apache Beam, Dataflow or similar Big Data solutions on Cloud Platforms.
    * Design and manage **batch and real-time data ingestion** from multiple data sources and messaging systems.
    * Collaborate with analytics/data science teams on data mart optimizations, query tuning, database design and Data Modeling.
    * Monitor performance and advise any necessary infrastructure changes
    * Modelling data and metadata to support ad-hoc and pre-built reporting
    * Work with product and engineering teams on different data driven products and drive/implement the Data computation pipeline for the same.
    * Own the design, development and maintenance of ongoing metrics, reports, dashboards etc on Data Platform to drive key business decisions.
    * 2+ years of industry experience in the Big Data stack.
    * Demonstrated ability in data modeling, ETL development, and data warehousing
    * Knowledge and exposure on Big Data solutions on cloud platforms like GCP/AWS/Azure
    * Experience with cloud MPP DW solutions like **Google BigQuery/Redshift/Azure SQL DW**
    * Experience with various messaging systems such as **Kafka, RabbitMQ, Kinesis** etc.
    * Knowledge of various ETL techniques and frameworks
    * Experience with **Python, Airflow, Spark SQL/NoSQL**.
    * Strong **SQL** and data modelling skills with solid knowledge of various industry standards such as **dimensional modelling, star schemas** etc
    * Experience with big data tools such as **Cloud Dataflow/BigQuery** etc.
    * Experience and exposure with GCP is Plus.
    * Good to have knowledge on PostgreSQL Databases
    * Familiarity with reporting tools like **Tableau or other BI packages**

* ola [Job_Description](https://www.linkedin.com/jobs/view/2821131595)
    * **Hadoop, Hbase, Hive, Kafka, Spark, Flink, Mesos/Kubernetes, Hudi/Deltalake , Prometheus, Grafana** etc.
    * 4+ years of relevant industry experience.
    * 2+ years experience in custom ETL design, implementation and maintenance.
    * Working knowledge of relational databases and query authoring (**SQL**).
    * Good Experience with **Java**(mandatory).
    * Working experience with data at the petabyte scale.
    * Experience designing, building and operating robust distributed systems.
    * Experience designing and deploying high performance systems with reliable monitoring and logging practices.
    * Effectively work across team boundaries to establish overarching data architecture, and provide guidance to individual teams.
    * Expertise of Amazon Web Services (**AWS**) and/or other relevant Cloud Infrastructure solutions like Microsoft Azure or Google Cloud.
    * Experience in managing and deploying containerized environments using **Docker, Mesos/Kubernetes** is a plus.
    * Experience in managing projects using scrum methodology.
    * Experience with multiple datastores is a plus (**MySQL, PostgreSQL, Aerospike, Couchbase, Scylla, Cassandra, Elasticsearch**). 

* ola_2 [Job_Description](https://www.linkedin.com/jobs/view/2956522512)
    * 3+ years of relevant industry experience.
    * 1+ years experience in custom ETL design, implementation and maintenance.
    * Working knowledge of relational databases and query authoring (SQL).
    * Good Experience with JAVA / Python / Scala / Spark.
    * Working experience with data pipelines.
    * Experience building and operating robust distributed systems.
    * Experience deploying high performance systems with reliable monitoring and logging practices.
    * Experience of Amazon Web Services (AWS) and/or other relevant Cloud Infrastructure solutions like Microsoft Azure or Google Cloud.
    * Experience in managing and deploying containerized environments using Docker, Mesos/Kubernetes is a plus.
    * Experience with multiple datastores is a plus (MySQL, PostgreSQL, Aerospike, Couchbase, Scylla, Cassandra, Elasticsearch).

* hotstar [Job_Description](https://www.linkedin.com/jobs/view/2793448281)
    * BE/B.Tech/BS/MS/PhD in Computer Science or a related field (ideal)
    * 3 -5 years of work experience building **data warehouse and BI systems**
    * Strong **Java** skills
    * Experience in either Go or **Python** (plus to have)
    * Experience in **Apache Spark, Hadoop, Redshift, Athena**
    * Strong understanding of database and storage fundamentals
    * Experience with the **AWS** stack
    * Ability to create data-flow design and write complex **SQL / Spark based transformations**
    * Experience working on real time streaming data pipelines using **Spark Streaming or Storm**

* cred [Job_Description](https://www.linkedin.com/jobs/view/2926152740)
    * 3 to 7 years of relevant experience in data engineering
    * experience in scripting with advanced sql, python/ scala and shell
    * a good understanding of spark/ hadoop/ other distributed systems
    * understanding of apache airflow and/ or nifi workflows
    * hands on experience in sql query tuning and data modelling
    * experience in administering visualization/ SQL workbench tools like Looker, Hue, Tableau, SuperSet etc
    * relevant working knowledge on any of the rdbms and aws redshift
    * knowledge of redshift administration skills
    * experience in cloud platform(s) (preferably aws)

* Truecaller [Job_Description](https://www.linkedin.com/jobs/view/2823679336)
    * Experience working with orchestration tools (eg: **Airflow**).
    * Experience working with big data and ETL development.
    * Programming skills in **SQL, Spark with Scala**
    * Experience working with cloud computing services (eg : **GCP, AWS, Azure**).
    * Experience with **Data Science workflows**.
    * Excellent problem solving and communication skills both with peers and experts from other areas.
    * Self-motivated and have a proven ability to take initiative to solve problems.
    * Work with vast projects like migration to **Google BigQuery**, migration to cloud-nativea technologies, etc.
    * Experience with **Kubernetes**.
    * Anomaly detection applications.
    * **Real-time validation of data**.
    * Experience with **Apache Beam, Apache Flink or CloudML**.

* [Flipkart](https://www.linkedin.com/company/flipkart/) [Job_Description](https://www.linkedin.com/jobs/view/2874955982)
    * 1-3 years’ experience with a Bachelor's Degree in Computer Science, Engineering, Technology or related field required. 1-2 years of relevant software development experience with sound skills in database modeling (relational, multi-dimensional) & optimization and data architecture - database
    * Experience with Enterprise Business Intelligence Platform/Data platform sizing, tuning, optimization and system landscape integration in large-scale, enterprise deployments.
    * Comfortable in one of the programming languages preferably Java, Scala or Python and Good knowledge of Agile, SDLC/CICD practices and tools.
    * Must have experience with Hadoop, Mapreduce, Hive, Spark, Scala programming. Must have good knowledge of performance tuning/optimizing data processing jobs, debugging time consuming jobs.
    * Proven experience in development of conceptual, logical, and physical data models for Hadoop, relational, EDW (enterprise data warehouse) and OLAP database solutions.
    * Experience on BI tool e.g. PowerBI will be desirable and Experience working extensively in multi-petabyte DW environment

* [Redbus](https://www.linkedin.com/company/redbus_2/) [Job_Description](https://www.linkedin.com/jobs/view/2873291049)
    * In-depth understanding of NoSql database systems like Cassandra, MongoDB
    * Working knowledge of key-value storage like Redis, RocksDB
    * High-level language proficiency like Python (including pandas) or Java (multi-threading and JVM tuning).
    * Stream-processing systems: Storm, Spark-Streaming, etc.
    * Experience in Data warehouse design and dimensional modeling
    * Experience with workflow automation with Airflow or Oozie
    * Experience in functional programming will be an added advantage.
    * Knowledge of serverless architectures (e.g. Lambda, Kinesis, Glue)
    * Ability to demonstrate micro / macro design and familiar with Unix Commands and basic work experience in Unix Shell Scripting.
    * Experience with AWS solutions such as EC2, RDS, ElasticCache, S3, Athena, Cloudwatch etc. will be an added advantage

* [Apna](https://www.linkedin.com/company/apnahq/jobs/) [Job_Description](https://www.linkedin.com/jobs/view/2873773630)
    * 3+ years of experience in a Data Engineer role.
    * Experience building and optimizing big data pipelines, architectures and data sets.
    * Build systems supporting data transformation, data structures, metadata, dependency and workload management.
    * Working knowledge of message queuing, stream processing, and highly scalable data stores.
    * A successful history of manipulating, processing and extracting value from large disconnected datasets.
    * Experience with big data tools: Hadoop, Spark, Kafka, etc.
    * Experience with relational SQL and NoSQL databases.
    * Experience with object-oriented/object function scripting languages: Python, Java, Scala, etc.
    * Experience with AWS/GCP cloud services: EC2, EMR, RDS, Redshift, DataProc, CloudSql, Bigquery

* [Nike](https://www.linkedin.com/company/nike/life/1e81eceb-ed3c-4441-805b-73f62728a415/) [Job_Description](https://www.linkedin.com/jobs/view/2870110020)
    * Able to quickly pick up new programming languages, technologies, and frameworks
    * Able to influence and communicate effectively, both verbally and written, with team members and business stakeholders
    * Has a strong problem solving and analytical mindset
    * Strong understanding of solution and technical design
    * Strong understanding of data structures and algorithms
    * Has a passion for data solutions In-depth knowledge of scalable cloud
    * Experience with workflow scheduling tools such as Airflow
    * Experience with source control tools such as GitHub and related dev process
    * Experience with scripting languages such as Shell, Python Experience with relational SQL
    * Experience building data lake solutions leveraging one or more of following AWS, EMR, S3, Hive & Spark
    * 2+ years' experience developing Data & Analytic solution

* [Ola Electric](https://www.linkedin.com/company/olaelectric/) [Job_Description](https://www.linkedin.com/jobs/view/2891105271)
    * Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working familiarity with a variety of databases.
    * Experience building and optimizing ‘big data’ data pipelines, architectures and data sets.
    * Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
    * Strong analytic skills related to working with unstructured datasets.
    * Build processes supporting data transformation, data structures, metadata, dependency and workload management.
    * A successful history of manipulating, processing and extracting value from large disconnected datasets.
    * Working knowledge of message queuing, stream processing, and highly scalable ‘big data’ data stores.
    * Strong project management and organizational skills.
    * Experience supporting and working with cross-functional teams in a dynamic environment.
    * They should also have experience using the following software/tools:
    * 
    * Experience with big data tools: Hadoop, Spark, Kafka, etc.
    * Experience with relational SQL and NoSQL databases, including Postgres and Cassandra.
    * Experience with data pipeline and workflow management tools: Azkaban, Luigi, Airflow, etc.
    * Experience with Azure cloud services: Synapse, ADF, ADX, Databricks, CosmosDB, Azure SQL DB, PostgreSQL
    * Experience with stream-processing systems: Storm, Spark-Streaming, etc.
    * Experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
    * Azure Certifications for AZ-900, DP-900, DP-200 & DP-201 (or DP-203) is a plus.

* [Classplus](https://www.linkedin.com/company/classplus/) [Job_Description](https://www.linkedin.com/jobs/view/2888049171)
    * Have good understanding of python, sql and shell scripting.
    * Have knowledge of implementing and maintaining data science model on scale Strong analytical skills, debugging and troubleshooting skills, product line analysis.
    * Have hands-on experience on GCP, Big query, Airbyte, cloud orchestrator, cloud storage and other GCP services.
    * Have proficiency in usage of tools like Kubernetes, Redash or any other visualization tools is good to have.
    * Are a follower of agile methodology (Sprint planning, working on JIRA, retrospective, etc).
    * Hands-on experience with the queueing system pub-sub preferred.
    * Knowledge on versioning like Git and deployment processes like CI CD.

* [Upstox](https://www.linkedin.com/company/upstox/) [Job_Description](https://www.linkedin.com/jobs/view/2853627926)
    * Must have: Hands-on working experience with Python / Scala, Spark(or similar frameworks), Big-data ETL pipelines, Hadoop (or similar distributed systems), advanced SQL, SQL query tuning.
    * Good to have: Working knowledge of streaming frameworks (Spark Streaming / Apache Flink or similar frameworks), workflow management tools / platforms (Apache Airflow / NiFior similar solutions), dimensional data modelling, cloud native solutions (preferably on AWS).
    * Brownie Points: Understanding of AWS Services -Redshift, DynamoDB, Lambda, Glue, Athena, Lake Formation, IAM, SQS & SNS, creating Cloud Formation / Terraform templates, understanding of Devops & SRE.
    * At least 3 years of experience with minimum 1.5 years of experience in a Data Engineering role with exposure to dealing with large volumes of data.

* Paytm [Job_Description](https://jobs.lever.co/paytm/181bee49-aca0-402c-a26b-0fff5f81a4f0)
    * You have previously worked on building serious data pipelines ingesting and transforming > 10 ^6 events per minute and terabytes of data per day.
    * You are passionate about producing clean, maintainable and testable code part of real-time data pipeline.
    * You understand how microservices work and are familiar with concepts of data modelling.
    * You can connect different services and processes together even if you have not worked with them before and follow the flow of data through various pipelines to debug data issues.
    * You have worked with Spark and Kafka before and have experimented or heard about Flink/Druid/Ignite/Presto/Athena and understand when to use one over the other.
    * On a bad day maintaining zookeeper and bringing up cluster doesn't bother you.
    * You may not be a networking expert but you understand issues with ingesting data from applications in multiple data centres across geographies, on-premise and cloud and will find a way to solve them.
    * Proficient in Java/Scala/Python/Spark

* [HashedIn](https://hashedin.com/) [Job_Description](https://hashedin.com/jobs/data-engineer/)
   * 2-8 years of experience
   * Strong coding/debugging/problem-solving abilities and should have advanced knowledge of at least one programming language - Python , Java , or Scala
   * Expertise in efficiently leveraging the power of distributed big data systems , including but not limited to Hadoop Hive , Spark , Kafka streaming etc.   
   * Experience in cloud-based data engineering service offerings from GCP/ AWS/ Azure like S3 , Redshift , Athena , Kinesis , etc.
   * Strong understanding of SQL , columnar databases , and additionally data warehousing concepts
   * building scalable and performant data pipelines , implementing custom ETLs , data lake , and data warehouse solutioning
   * Familiar with any common data visualization and exploration tools (Tableau , AWS Quicksight , Looker , etc.) 
   * Hadoop , Hive , Spark , HBase , Kafka streaming , Tableau , Airflow , and other cloud-based data engineering services like S3 , Redshift , Athena , Kinesis , etc. 

### other startups, unicorns to check
* https://www.linkedin.com/pulse/linkedin-top-startups-2021-25-indian-companies-rise-/
* ixigo
* juspay
* paypal
* slice
* Tata1mg
* Dunzo

### Data Analyst
* [Truecaller](https://www.linkedin.com/jobs/view/2831863910)
* [Twitter](https://www.linkedin.com/jobs/view/2848234827)
* [Myntra](https://www.linkedin.com/jobs/view/2849258558)

<h3> Domains for fullstack </h3>
<ul>
  <li> ETL </li>
  <li> Visualization </li>
  <li> Cloud </li>
</ul>

### skills Required (for de)
- Airflow
- Spark
- Snowflake (interview questions)
- Terraform
- CRON -- learn how to write cron notation time
- Unix  
- sql (window functions, surrogate key, lag functions, stored procedures)
- mongodb, cassandra, hbase, dynamodb, s3
- tableu / powerbi
- Docker/ [Kubernetes](https://www.youtube.com/watch?v=X48VuDVv0do)
- AWS (Tech: )
- Azure (Tech: Databricks, ADF, Adls)
- Git, github

### skills Required (for da)
- statistics basics




[how_to_write_md_files](https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)

[data_engg_repo](https://github.com/siddhaant-jain/data-engineering-fullstack/blob/master/README.md)

[airflow_playlist_1](https://www.youtube.com/playlist?list=PLYizQ5FvN6pvIOcOd6dFZu3lQqc6zBGp2)
[airflow_playlist_2](https://www.youtube.com/playlist?list=PLzKRcZrsJN_xcKKyKn18K7sWu5TTtdywh)
[Tableu_course](https://www.simplilearn.com/skillup-free-online-courses)

docker repository link: https://github.com/siddhaant-jain/Docker-Notes-and-Resources
