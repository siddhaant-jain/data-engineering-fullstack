# data-engineering-fullstack
This repository contains links to all resources for full stack data engineering skillset

<h3> List of Target Companies </h3>

* [Razorpay] (https://razorpay.com/?utm_source=linkedin&utm_medium=social) [Job_Description](https://www.linkedin.com/jobs/view/2631333149)
    * ranked in linkedin top startups
    * neelesh told to get knowledge of financial domain, trivia and stuff also
    * Experience building and optimizing **‘big data’ data pipelines, architectures and data sets**.
    * Strong **analytic skills** related to working with structured/unstructured datasets.
    * Build processes supporting data transformation, data structures, **dimensional modelling, metadata, dependency, schema registration/evolution and workload managemen**t.
    * Working knowledge of **message queuing, stream processing**, and highly scalable ‘big data’ data stores.
    * Experience supporting and working with cross-functional teams in a dynamic environment.
    * We are looking for a candidate with 4+ years of experience in a Data Engineer role. They should also have experience using the following software/tools:
        * Experience with big data tools: **HDFS/S3, Spark/Flink,Hive,Hbase, Kafka/kanisis**, etc.
        * Experience with relational SQL and NoSQL databases, including **Elasticsearch and Cassandra/Mongodb**.
        * Experience with data pipeline and workflow management tools: **Azkaban, Luigi, Airflow**, etc.
        * Experience with **AWS** /GCP cloud services
        * Experience with stream-processing systems: **Spark-Streaming/Flink** etc.
        * Experience with object-oriented/object function scripting languages: **Java, Scala**, etc.

* [Disruptors_Capital](http://www.disruptors.capital/) [Job_Description](https://www.linkedin.com/jobs/view/2850688238)
    * Very small company, recently started, less than 100 employees, investment company
    * Experience building scalable, real time and high-performance data lake solutions usingtech such **Kafka**, **AWS EMR, S3, Hive, Spark**- Experience with workflow scheduling tools like **Airflow**- Experience with relational and non-relational databases- Experience with scripting languages such as Shell, Python- Programming Languages: **Java / Python / Scala**- Experience working with micro service architecture- Experience working with cloud-based service providers (preferably AWS)- Knowledge of **linear and logistic regression, K-means, K-medoids, Naive Bayes, Decision tree, Random forest, Clustering, Bayesian model, CNN, Transformers** would be considered a plus- Knowledge of natural language processing (**NLP**) will be considered a plus
* [Walmart_Global_Tech_India](https://one.walmart.com/content/globaltechindia/en_in.html) [Job_Description](https://www.linkedin.com/jobs/view/2848676337) 
    * Bachelor's Degree or Master’s Degree with **3+ years** of experience in Computer Science or related field.
    * Experience in **ETL** and Expertise in **SQL**.
    * Expertise in Big Data Ecosystem with experience in **Hadoop, Hive, Spark, Strome, Cassandra, NoSQL DB’s**.
    * Expertise in **MPP architecture** and knowledge of MPP engine (Spark, **Impala** etc).
    * Data pipeline/workflow management tools such as **Azkaban, Airflow and oozie**
    * **Cloud** Development experience
    * Experience in building scalable/highly available distributed systems in production.
    * Understanding of **stream processing** with knowledge on **Kafka**.
    * Knowledge of Software Engineering best practices with experience on implementing CI/CD, Log aggregation/* * Monitoring/alerting for production system.
    * Very good expertise in production support related activities (issue identification, resolution)
* [Goldman_Sachs](http://www.goldmansachs.com/) [Job_Description_1](https://www.linkedin.com/jobs/view/2813599926) [Job_Description_2](https://www.linkedin.com/jobs/view/2814202082)
    * Advanced degree in Engineering, Computer Science or related disciplines
    * Hands-on experience with one or more mainstream programming languages (**Python, Java, C, Scala, C++**, etc.)
    * Hands-on experience with one or more relational database systems (**Sybase, Snowflake, Oracle**)
    * Experience in building data products from ideation to implementation
    * At least 2 years experience in a visualization tool such as **Tableau / QlikView / Spotfire / Power BI**
    * At least 3 years experience dealing with data (structured or unstructured)
    * Ability to use data for business analysis and to drive customer focussed ideas
    * Strong project management skills
    * Experience working in a start-up business or a new business line within a larger organization is preferred

* [Twilio](http://www.twilio.com/) [Job_Description](https://www.linkedin.com/jobs/view/2848216345) 
    * Product based (Chat App), IPO done, 11 rounds funding done
    * Minimum of 3+ years of technical experience supporting highly-available Data warehouse systems and ETLs.
    * Hands-on strong experience with **SQL/NoSQL** databases. A good understanding of troubleshooting highly **scalable Data warehouses and Data pipelines**.
    * Hands on Experience in **SQL , Python and Scala** programming languages.
    * Experience in supporting distributed environments using **Spark, Hive, Presto, Hadoop** etc.
    * Deep understanding of architecture and functioning of Distributed database systems like **Snowflake, Presto and Redshift** (any ) .
    * Experience working with various file formats like **Parquet, Avro, ORC** for large volumes of data
    * Experience with one or more NoSQL databases such as **Cassandra, MongoDB, DynamoDB** is a plus.
    * Strong understanding of engineering standard methodologies design principles
    * Experience working in agile environment and iterative development

* [Baker_Hughes](http://www.bakerhughes.com/) [Job_Description](https://www.linkedin.com/jobs/view/2849489228)
    * Baker Hughes (NASDAQ: BKR) is an energy technology company that provides solutions for energy and industrial customers worldwide.
    * Be a Graduate in Computer Engineering. A minimum 3 yrs of professional experience
    * Have minimum of 3 yrs of development in **Informatica or Talend**
    * Have minimum of 2 yrs of writing **advanced SQL and stored procedures**
    * Have Understanding of logical and physical **data models and data lineage**
    * Have good experience with SQL Database such as **Oracle MySQL, PostgreSQL**
    * Have hands-on experience with **star schema models** to convert and derive flat summary data models for analytics and summary statistics
    * Have experience with **cloud** technologies and open source tools like **python, spark, Java**
    * Have experience on new D&A application development and existing application optimization is required (data source identification, mapping, ingestion, modelling and transformation for analytical consumption purposes).

* [Hitachi_Vantara](Hitachi Vantara) [Job_Description](https://www.linkedin.com/jobs/view/2850487966)
    * wholly-owned subsidiary of Hitachi, Ltd., guides our customers from what’s now to what’s next by solving their digital challenges, More than 80% of the Fortune 100 trust Hitachi Vantara to help them develop new revenue stream
    * 5+ years' experience working in data & analytics ecosystem, working with structured, semi-structured and unstructured data.
    * Demonstrated experience with **Data Pipeline development** both on-premises and in the cloud (AWS/Azure) using open-source technology
    * Responsible for creating reusable and scalable data pipelines using **cloud ETL tools & technology like Azure Data Factory, Glue etc.**
    * Sound understanding of various data solution patterns and when to use them: **ETL/ELT, RDBMS, Normalization/De-normalization, Key-Value, In-Memory, Wide Column, Columnar, Text Indexing, Streaming, Messaging**
    * Strong software engineering and object-oriented programming skills with expertise in languages such as **SQL, Python.**
    * Strong **data warehousing, data modeling and data manipulation skills**, such as in SQL, Stored Procedures, SQL Server
    * Experience with Agile methodologies, such as Scrum, Kanban, Lean
    * Experience building data pipelines using **Databricks**.
    * Experience building on emerging cloud serverless managed services, to minimize/eliminate physical/virtual server footprint
    * Experience implementing security solutions for data storage and processing in the cloud

* [Zoom](https://www.zoom.us/) [Job_Description](https://www.linkedin.com/jobs/view/2848214174)
    * famous video call platform 
    * jd not given

* [Expedia_Group](http://www.lifeatexpediagroup.com/) [Job_Description](https://www.linkedin.com/jobs/view/2849027140)
    * travel company (product based)
    * this jd for 8 years experience

* [Trell](https://trell.co.in/) [Job_Description](https://www.linkedin.com/jobs/view/2848222710)
    * linkedin ranked top startup
    -Experience in building production big-data crunching systems (**Spark, Kafka, HIVE, Flink** etc)
    * Strong data architectural skills.
    * Strong problem-solving skills.
    * Knowledge of distributed computing using **Spark**.
    * Ability to work in cross-functional projects and drive decision making.
    * Ability to wrangle unstructured data, as per use cases.
    * Experience schema design, data modelling and manipulating large data sets using both relational (e.g., **MySQL Postgres**) and non-relational databases (e.g., **MongoDB, CouchDB, Redis, Cassandra**).
    * Ability to consume, transform and optimize complex SQL queries.
    * Solid knowledge in python and data structures & a software engineering mindset, striving to write elegant, maintainable code.
    * Reliable knowledge/experience in consuming Web Services.
    * Knowledge/experience in consuming **real-time data streams**.
    * Knowledge of data warehousing (**BigQuery, Redshift, Snowflake** etc.) and working with data lakes (**S3, ADLS**).
    * Knowledge of **statistical** and **machine learning** models and preprocessing techniques would be an added advantage

* Thermo Fisher [Job_Description](https://www.linkedin.com/jobs/view/2848250066)
    * Having Certifications like **AWS Certified Data Analytics, CCA Spark and Hadoop Developer or CCP Data Engineer** is highly desirable
    * 5+ Years of Overall IT Experience and at least 3+ years as AWS Data Engineer in Data Lake, Data Analytics & Business Intelligence Solutions
    * Full life cycle project implementation experience in AWS using **Pyspark/EMR, Athena, S3, Redshift, AWS API Gateway, Lambda, Glue** and other managed services
    * Strong experience in building ETL data pipelines using Pyspark on EMR framework
    * Hands on experience in using S3, AWS Glue jobs, S3 Copy, Lambda and API Gateway.
    * Working SQL experience to troubleshoot SQL code. Redshift knowledge is an added advantage.
    * Hands-on with system & application log tools like **Datadog, CloudWatch, Splunk** etc.
    * Experience working with Python, **Python ML libraries** for data analysis, wrangling and insights generation
    * Strong understanding of **AWS Data lake and data bricks**.
    * Exposure to **Kafka, Redshift, Sage Maker** would be added advantage
    * Exposure to data visualization tools like **Power BI, Tableau** etc.

* [Grofers](http://www.blinkit.in/) [Job_Description](https://www.linkedin.com/jobs/view/2848216923)
    * 6 to 8 years of relevant industry experience.
    * Bachelor's and/or Master's degree in CS, or equivalent experience.
    * Solid knowledge of Computer Science fundamentals (object-oriented design, **data structures** and algorithm design, problem-solving, and complexity analysis).
    * A deep understanding of programming with one or more programming languages.
    * Python and Scala are preferred.
    * Experience designing, building and operating distributed systems, deploying high-performance systems with reliable monitoring and logging practices
    * Experience operating big data systems for large scale storage and processing of data.
    * Experience leading projects and mentoring members in the team.
    * Effectively work across team boundaries to establish overarching data architecture, and provide guidance to individual teams.
    * In-depth knowledge of relational and non-relational databases.
    * Take part in code/design reviews to further develop your sense of code quality, maintainability, test coverage, and good design
    * **Data Analysis,Data Streaming,NoSQL,Search,Kafka**

* [Agoda](http://careersatagoda.com/) [Job_Description](https://www.linkedin.com/jobs/view/2758702161)
    * Bangkok based job, online travel platform
    * Over 2 years of hands-on experience working with technology like Spark, Hadoop, Kafka
    * Knowledge and experience with machine learning
    * Experience with Scrum/Agile development methodologies
    * Experience building large-scale distributed products
* American_Express [Job_Description](https://www.linkedin.com/jobs/view/2766245216)
* Visa [Job_Description](https://www.linkedin.com/jobs/view/2814744862)
* slice
* [Bosch](http://www.boschindia.com/) [Job_Description](https://www.linkedin.com/jobs/view/2849256751)
    * Hands on experience on **Spark sql and Spark streaming**.
    * Hands on experience on **Scala** language.
    * In depth knowledge of **SQL** and **NoSQL** (HBase).
    * Good in writing Shell and **python** script.
    * Good understanding of **Hadoop ecosystem**.
    * Hands on experience on InteliJ, Github /Bitbucket, HUE.
    * Understanding of **ETL** proces

* [Acko](http://www.acko.com/) [Job_Description](https://www.linkedin.com/jobs/view/2818948287)
    * product based, insurance, d-series funding
    * Design, develop and maintain scalable data pipelines with a focus on writing clean, fault-tolerant code using Python, Airflow, Kafka, Spark, Apache Beam, Dataflow or similar Big Data solutions on Cloud Platforms.
    * Design and manage **batch and real-time data ingestion** from multiple data sources and messaging systems.
    * Collaborate with analytics/data science teams on data mart optimizations, query tuning, database design and Data Modeling.
    * Monitor performance and advise any necessary infrastructure changes
    * Modelling data and metadata to support ad-hoc and pre-built reporting
    * Work with product and engineering teams on different data driven products and drive/implement the Data computation pipeline for the same.
    * Own the design, development and maintenance of ongoing metrics, reports, dashboards etc on Data Platform to drive key business decisions.
    * 2+ years of industry experience in the Big Data stack.
    * Demonstrated ability in data modeling, ETL development, and data warehousing
    * Knowledge and exposure on Big Data solutions on cloud platforms like GCP/AWS/Azure
    * Experience with cloud MPP DW solutions like **Google BigQuery/Redshift/Azure SQL DW**
    * Experience with various messaging systems such as **Kafka, RabbitMQ, Kinesis** etc.
    * Knowledge of various ETL techniques and frameworks
    * Experience with **Python, Airflow, Spark SQL/NoSQL**.
    * Strong **SQL** and data modelling skills with solid knowledge of various industry standards such as **dimensional modelling, star schemas** etc
    * Experience with big data tools such as **Cloud Dataflow/BigQuery** etc.
    * Experience and exposure with GCP is Plus.
    * Good to have knowledge on PostgreSQL Databases
    * Familiarity with reporting tools like **Tableau or other BI packages**

* ola [Job_Description](https://www.linkedin.com/jobs/view/2821131595)
    * **Hadoop, Hbase, Hive, Kafka, Spark, Flink, Mesos/Kubernetes, Hudi/Deltalake , Prometheus, Grafana** etc.
    * 4+ years of relevant industry experience.
    * 2+ years experience in custom ETL design, implementation and maintenance.
    * Working knowledge of relational databases and query authoring (**SQL**).
    * Good Experience with **Java**(mandatory).
    * Working experience with data at the petabyte scale.
    * Experience designing, building and operating robust distributed systems.
    * Experience designing and deploying high performance systems with reliable monitoring and logging practices.
    * Effectively work across team boundaries to establish overarching data architecture, and provide guidance to individual teams.
    * Expertise of Amazon Web Services (**AWS**) and/or other relevant Cloud Infrastructure solutions like Microsoft Azure or Google Cloud.
    * Experience in managing and deploying containerized environments using **Docker, Mesos/Kubernetes** is a plus.
    * Experience in managing projects using scrum methodology.
    * Experience with multiple datastores is a plus (**MySQL, PostgreSQL, Aerospike, Couchbase, Scylla, Cassandra, Elasticsearch**). 

* hotstar [Job_Description](https://www.linkedin.com/jobs/view/2793448281)
    * BE/B.Tech/BS/MS/PhD in Computer Science or a related field (ideal)
    * 3 -5 years of work experience building **data warehouse and BI systems**
    * Strong **Java** skills
    * Experience in either Go or **Python** (plus to have)
    * Experience in **Apache Spark, Hadoop, Redshift, Athena**
    * Strong understanding of database and storage fundamentals
    * Experience with the **AWS** stack
    * Ability to create data-flow design and write complex **SQL / Spark based transformations**
    * Experience working on real time streaming data pipelines using **Spark Streaming or Storm**

* Truecaller [Job_Description](https://www.linkedin.com/jobs/view/2823679336)
    * Experience working with orchestration tools (eg: **Airflow**).
    * Experience working with big data and ETL development.
    * Programming skills in **SQL, Spark with Scala**
    * Experience working with cloud computing services (eg : **GCP, AWS, Azure**).
    * Experience with **Data Science workflows**.
    * Excellent problem solving and communication skills both with peers and experts from other areas.
    * Self-motivated and have a proven ability to take initiative to solve problems.
    * Work with vast projects like migration to **Google BigQuery**, migration to cloud-nativea technologies, etc.
    * Experience with **Kubernetes**.
    * Anomaly detection applications.
    * **Real-time validation of data**.
    * Experience with **Apache Beam, Apache Flink or CloudML**.



### other startups, unicorns to check
* https://www.linkedin.com/pulse/linkedin-top-startups-2021-25-indian-companies-rise-/
* ixigo
* juspay
* paypal
* slice
* Tata1mg
* Dunzo

### Data Analyst
* [Truecaller](https://www.linkedin.com/jobs/view/2831863910)
* [Twitter](https://www.linkedin.com/jobs/view/2848234827)
* [Myntra](https://www.linkedin.com/jobs/view/2849258558)

<h3> Domains for fullstack </h3>
<ul>
  <li> ETL </li>
  <li> Visualization </li>
  <li> Cloud </li>
</ul>

### skills Required (for de)
- Airflow
- Spark
- Snowflake (interview questions)
- Terraform
- CRON -- learn how to write cron notation time
- Unix  
- sql (window functions, surrogate key, lag functions, stored procedures)
- mongodb, cassandra, hbase, dynamodb, s3
- tableu / powerbi
- Docker/ [Kubernetes](https://www.youtube.com/watch?v=X48VuDVv0do)
- AWS (Tech: )
- Azure (Tech: Databricks, ADF, Adls)
- Git, github

### skills Required (for da)
- statistics basics




[how_to_write_md_files](https://docs.github.com/en/github/writing-on-github/getting-started-with-writing-and-formatting-on-github/basic-writing-and-formatting-syntax)

[data_engg_repo](https://github.com/siddhaant-jain/data-engineering-fullstack/blob/master/README.md)

[airflow_playlist_1](https://www.youtube.com/playlist?list=PLYizQ5FvN6pvIOcOd6dFZu3lQqc6zBGp2)
[airflow_playlist_2](https://www.youtube.com/playlist?list=PLzKRcZrsJN_xcKKyKn18K7sWu5TTtdywh)
[Tableu_course](https://www.simplilearn.com/skillup-free-online-courses)

docker installation for windows:
- https://docs.docker.com/desktop/windows/install/
- https://hub.docker.com/editions/community/docker-ce-desktop-windows/

- https://docs.docker.com/compose/install/
- docker compose part of docker desktop
